{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37133fb0",
   "metadata": {},
   "source": [
    "# Attention Model Documentation\n",
    "\n",
    "This notebook serves as a documentation hub for understanding each part of the code presented by Dmitry Eremeev and Alexey Pustynnikov.\n",
    "\n",
    "## Table of Contents (Files)\n",
    "\n",
    "1. [`train_model.ipynb`](#train_model)\n",
    "2. [`train.py`](#train)\n",
    "3. [`attention_graph_encoder.py`](#graph_encoder)\n",
    "4. [`attention_dynamic_model.py`](#dynamic_model)\n",
    "5. [`reinforce_baseline.py`](#baseline)\n",
    "6. [`environment.py`](#environment)\n",
    "7. [`layers.py`](#layers)\n",
    "8. [`utils.py`](#utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79c581",
   "metadata": {},
   "source": [
    "# `train_model.ipynb`\n",
    "\n",
    "The notebook contains the code for:\n",
    "\n",
    "1. setting up.\n",
    "2. Loading/Creating data\n",
    "3. Executing experiment\n",
    "4. Showing learning loss\n",
    "\n",
    "## Parameter set up\n",
    "\n",
    "The notebook is divided into two cells that execute the code either not saving or saving the resulting models and loading it up. On either cell, the first part of code we can appreciate is the parameters for the model to be used. The outlined parameters here are:\n",
    "\n",
    "``` python\n",
    "SAMPLES = 512 \n",
    "BATCH = 128\n",
    "START_EPOCH = 0\n",
    "END_EPOCH = 5\n",
    "FROM_CHECKPOINT = False\n",
    "embedding_dim = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "ROLLOUT_SAMPLES = 10000\n",
    "NUMBER_OF_WP_EPOCHS = 1 \n",
    "GRAD_NORM_CLIPPING = 1.0 \n",
    "BATCH_VERBOSE = 1000\n",
    "VAL_BATCH_SIZE = 1000\n",
    "VALIDATE_SET_SIZE = 10000\n",
    "SEED = 1234\n",
    "GRAPH_SIZE = 50\n",
    "FILENAME = 'VRP_{}_{}'.format(GRAPH_SIZE, strftime(\"%Y-%m-%d\", gmtime()))\n",
    "```\n",
    "\n",
    "Some extra notes on these parameters:\n",
    "\n",
    "```python \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "<a id='train_model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69d803",
   "metadata": {},
   "source": [
    "# `train.py`\n",
    "\n",
    "\n",
    "\n",
    "<a id='train'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0cd260",
   "metadata": {},
   "source": [
    "# `attention_graph_encoder.py`\n",
    "\n",
    "<a id='graph_encoder'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb806d2",
   "metadata": {},
   "source": [
    "# `attention_dynamic_model.py`\n",
    "\n",
    "The dynamic model's main code is hosted here. The rest of the model's lower-level parts can be found in [`attention_graph_encoder.py`](#graph_encoder) and [`layers.py`](#layers). Further, the model's class makes use directly of the environment problem to be trained on. These definitions can be found at [`environment.py`](#environment)\n",
    "\n",
    "![Encoder Decoder Architecture](pictures/EncoderDecoderAM.png)\n",
    "\n",
    "## Constructor\n",
    "\n",
    "The main thing to keep in mind is that the constructor does the following:\n",
    "\n",
    "- Sets up the basic parameters of the Attention Model. (embedding dimensions etc.)\n",
    "- Creates the Encoder modules.\n",
    "- Sets up the projections as defined in the paper as Dense layers.\n",
    "\n",
    "### Input Parameters\n",
    "\n",
    "`embedding_dim`\n",
    "\n",
    "`n_encode_layers`\n",
    "\n",
    "`n_heads`\n",
    "\n",
    "`tanh_clipping`\n",
    "\n",
    "### Variables to keep in mind\n",
    "\n",
    "`embedding_dim`\n",
    "* MHA's embedding dimensions. Note that this will not be the actual embedding used in embedding the input. The embedding shall be: Number of Heads * Depth of Head. This is this way since instead of having individual projections in a list being applied to the same output, the output as a whole is passed through a single projection that applies it at the same time. The outputs of the projection would then be this embedding dimension, and it is easily \"split\" to allocate multi-headed attention. (Like folding down the dimensions.)\n",
    "\n",
    "`n_encode_layers`\n",
    "* Number of Encode layers to use. \n",
    "\n",
    "`decode_type`\n",
    "* Style of decoding. Either 'sampling' or 'greedy'.\n",
    "\n",
    "`problem`\n",
    "* Defines the problem type. Helpful in obtaining the mask and knowing when the problem is solved.\n",
    "\n",
    "`n_heads`\n",
    "* Number of heads in multi-headed attention.\n",
    "\n",
    "`embedder`\n",
    "* Embedder module. Type `GraphAttentionEncoder`. It encapsulates all 'Nx' encoders\n",
    "\n",
    "`output_dim`\n",
    "* Output dimensions of encoder. Used for the decoder. Same as `embedding_dim`\n",
    "\n",
    "`num_heads`\n",
    "* Number of heads. This is mathematically related to `embedding_dim`\n",
    "\n",
    "`head_depth` **?**\n",
    "* \"Actual\" embedding dimensions as described in original Transformer. Same as the dimension of the key, value,\n",
    "\n",
    "`dk_mha_decoder`\n",
    "* \n",
    "\n",
    "\n",
    "`dk_get_loc_p`\n",
    "\n",
    "\n",
    "`tanh_clipping`\n",
    "\n",
    "#### Linear Transformation\n",
    "\n",
    "In the literature, the query, key, and values are generated by applying a linear transformation on the embedded input nodes. These lienar projections are parametrized using a weight matrix for each: \n",
    "\n",
    "$$\n",
    "W^Q, \\; W^K, \\; W^V\n",
    "$$\n",
    "\n",
    "In the case of the Decoder, the Query is computed using not the embedded input but a context vector of the graph encoding, first, and last nodes produced:\n",
    "\n",
    "$$\n",
    "h^{(N)}_{(c)} = [\\bar{h}^{(N)},\\;\\; \\text{Last},\\;\\; \\text{First}]\n",
    "$$\n",
    "$$\n",
    "q_{(c)} = W^{Q}h_{(c)}\n",
    "$$\n",
    "\n",
    "Because the context input does not change but the last and first nodes does, it can be benefitial to compute the context matrix and recompute the first and last parts as needed. To do this, the matrix $W^Q_{\\text{context}}$ and $W^Q_{\\text{step\\_context}}$ are used.\n",
    "\n",
    "`wq_context`\n",
    "* Dense layer used for computing the context part of the query matrix.\n",
    "\n",
    "`wq_step_context`\n",
    "* Dense layer used for computing the first and last parts. In the case of VRP, the current node and left over material $[\\text{last}, \\;D_t]$\n",
    "\n",
    "`wk`\n",
    "* Matrix for calculating Key in decoder\n",
    "\n",
    "`wk_tanh`\n",
    "* Matrix used for calculating key in last 1HA in decoder\n",
    "\n",
    "`wv`\n",
    "* Used in calculating the value in MHA of decoder\n",
    "\n",
    "`w_out`\n",
    "* \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a id='dynamic_model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c938d8",
   "metadata": {},
   "source": [
    "# `reinforce_baseline.py`\n",
    "\n",
    "<a id='baseline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec93747",
   "metadata": {},
   "source": [
    "# `environment.py`\n",
    "\n",
    "<a id='environment'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1541c3",
   "metadata": {},
   "source": [
    "# `layers.py`\n",
    "\n",
    "<a id='layers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd76c458",
   "metadata": {},
   "source": [
    "# `utils.py`\n",
    "\n",
    "<a id='utils'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef741d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
