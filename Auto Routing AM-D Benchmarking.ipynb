{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a68e98fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 08:56:55.318256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "env: TF_GPU_ALLOCATOR=cuda_malloc_async\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 08:56:57.363621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 08:56:57.395975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 08:56:57.396151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "from attention_dynamic_model import AttentionDynamicModel\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from reinforce_baseline import RolloutBaseline\n",
    "from time import strftime, gmtime\n",
    "from utils import create_data_on_disk\n",
    "from train import train_model\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "%env TF_GPU_ALLOCATOR=cuda_malloc_async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cefaca",
   "metadata": {},
   "source": [
    "Benchmarking AM-D\n",
    "=================\n",
    "\n",
    "This experiment will train an AM-D model based on the following hyperparameters:\n",
    "\n",
    "| Hyperparameter | Values |\n",
    "| ---: | :--- |\n",
    "| Learning Rate | 1e-4, 5e-5 1e-5 |\n",
    "| Embedding Dimensions | 64, 128, 256 |\n",
    "| Attention Heads | 6, 8, 10 |\n",
    "| Encoder Layers | 1, 2, 3, 4 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a6e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 08:56:57.413515: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001\n",
      "Embedding Dimension: 64\n",
      "Attention Heads: 4\n",
      "Encoder Layers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 08:56:57.414051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 08:56:57.414414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 08:56:57.414680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 08:56:57.909216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 08:56:57.909418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 08:56:57.909575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-25 08:56:57.909673: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-10-25 08:56:57.909757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4566 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline model on baseline dataset (epoch = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rollout greedy execution: 100%|█████████████████| 10/10 [00:11<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2\n",
      "Current decode type: sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch calculation at epoch 0: 1it [00:01,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_global_norm = 4.556308269500732, clipped_norm = 0.9999999403953552\n",
      "Epoch 0 (batch = 0): Loss: -2.090458869934082: Cost: 13.4697847366333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch calculation at epoch 0: 79it [01:36,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating candidate model on baseline dataset (callback epoch = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rollout greedy execution: 100%|█████████████████| 10/10 [00:05<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 candidate mean 9.84041976928711, baseline epoch 0 mean 16.721332550048828, difference -6.880912780761719\n",
      "p-value: 0.0\n",
      "Update baseline\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' attention_dynamic_model_2/graph_attention_encoder_2/multi_head_attention_layer_4/MHA/dense_17/kernel:0': Shape mismatch.The variable shape (64, 64), and the assigned value shape (65, 64) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 88\u001b[0m\n\u001b[1;32m     78\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints/AMD-banchmarking-trained_on-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_iter \u001b[38;5;241m*\u001b[39m num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-num_layers-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menc_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-attn-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_h\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-embedding_dim-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menc_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-lr-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-date-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ckp\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     80\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m create_data_on_disk(\n\u001b[1;32m     81\u001b[0m     graph_size     \u001b[38;5;241m=\u001b[39mgraph_size,\n\u001b[1;32m     82\u001b[0m     num_samples    \u001b[38;5;241m=\u001b[39mnum_samples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     seed           \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     87\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_tf\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_amd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_epoch\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_checkpoint\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_norm_clipping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_norm_clipping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_verbose\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_size\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgraph_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m model_amd\u001b[38;5;241m.\u001b[39msave_weights(filename)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# To Do: Add information about progress\u001b[39;00m\n",
      "File \u001b[0;32m~/coding/github/DynamicAttentionModel-VRP/train.py:104\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(optimizer, model_tf, baseline, validation_dataset, samples, batch, val_batch_size, start_epoch, end_epoch, from_checkpoint, grad_norm_clipping, batch_verbose, graph_size, filename)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (batch = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m): Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: Cost: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_batch, epoch_loss_avg\u001b[38;5;241m.\u001b[39mresult(), epoch_cost_avg\u001b[38;5;241m.\u001b[39mresult()))\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Update baseline if the candidate model is good enough. In this case also create new baseline dataset\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m \u001b[43mbaseline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m set_decode_type(model_tf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Save model weights\u001b[39;00m\n",
      "File \u001b[0;32m~/coding/github/DynamicAttentionModel-VRP/reinforce_baseline.py:194\u001b[0m, in \u001b[0;36mRolloutBaseline.epoch_callback\u001b[0;34m(self, model, epoch)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p_val \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.05\u001b[39m:\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdate baseline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 194\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_baseline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# alpha controls the amount of warmup\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[0;32m~/coding/github/DynamicAttentionModel-VRP/reinforce_baseline.py:114\u001b[0m, in \u001b[0;36mRolloutBaseline._update_baseline\u001b[0;34m(self, model, epoch)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m load_tf_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_to_checkpoint,\n\u001b[1;32m    111\u001b[0m                                embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim,\n\u001b[1;32m    112\u001b[0m                                graph_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_size)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mcopy_of_tf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mgraph_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# For checkpoint\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline_checkpoint_epoch_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename), save_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/coding/github/DynamicAttentionModel-VRP/reinforce_baseline.py:31\u001b[0m, in \u001b[0;36mcopy_of_tf_model\u001b[0;34m(model, embedding_dim, graph_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m new_model(data_random)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(new_model\u001b[38;5;241m.\u001b[39mvariables, model\u001b[38;5;241m.\u001b[39mvariables):\n\u001b[0;32m---> 31\u001b[0m     \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_model\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:931\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    930\u001b[0m     tensor_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 931\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    932\u001b[0m       (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot assign value to variable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: Shape mismatch.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe variable shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, and the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massigned value shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are incompatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    935\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forward_compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m23\u001b[39m):\n\u001b[1;32m    937\u001b[0m   \u001b[38;5;66;03m# If the shape is fully defined, we do a runtime check with the shape of\u001b[39;00m\n\u001b[1;32m    938\u001b[0m   \u001b[38;5;66;03m# value.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot assign value to variable ' attention_dynamic_model_2/graph_attention_encoder_2/multi_head_attention_layer_4/MHA/dense_17/kernel:0': Shape mismatch.The variable shape (64, 64), and the assigned value shape (65, 64) are incompatible."
     ]
    }
   ],
   "source": [
    "learn_rates = [1e-4, 5e-5, 1e-5]\n",
    "enc_dims    = [64, 128, 256]\n",
    "attn_heads  = [4, 8] # Note: enc_dims must be divided by attn_heads\n",
    "enc_layers  = [1, 2, 3, 4]\n",
    "iterations  = [2, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "''' Constant Parameters '''\n",
    "# AM-D Constants\n",
    "tanh_clipping = 10\n",
    "\n",
    "# Optimizer Constants (Adam)\n",
    "beta_1        = 0.9\n",
    "beta_2        = 0.999\n",
    "epsilon       = 1e-07\n",
    "amsgrad       = False\n",
    "name          = \"Adam\"\n",
    "\n",
    "# Environment Constants\n",
    "graph_size         = 20\n",
    "\n",
    "# Rollout Baseline Constants\n",
    "wp_n_epochs        = 5\n",
    "epoch              = 0\n",
    "num_samples        = 10_000 #Validation Samples\n",
    "warmup_exp_beta    = 0.8\n",
    "\n",
    "# Training Constants\n",
    "samples            = 10_000 #1_280_000 # 512\n",
    "batch              = 128\n",
    "val_batch_size     = 1_000 # out of num_samples\n",
    "start_epoch        = 0\n",
    "grad_norm_clipping = 1.0\n",
    "batch_verbose      = 1_000\n",
    "\n",
    "global_start_time = time.time()\n",
    "\n",
    "for lr in learn_rates:\n",
    "    for enc_dim in enc_dims:\n",
    "        for attn_h in attn_heads:\n",
    "            for enc_layer in enc_layers:\n",
    "                print(f'Learning Rate: {lr}\\nEmbedding Dimension: {enc_dim}\\nAttention Heads: {attn_h}\\nEncoder Layers: {enc_layer}')\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Build AM-D Model\n",
    "                model_amd = AttentionDynamicModel(\n",
    "                    embedding_dim  =enc_dim,\n",
    "                    n_encode_layers=enc_layer,\n",
    "                    n_heads        =attn_h,\n",
    "                    tanh_clipping  =tanh_clipping\n",
    "                )\n",
    "                model_amd.set_decode_type('sampling')\n",
    "                \n",
    "                # Create Optimizer\n",
    "                optimizer = Adam(\n",
    "                    learning_rate=lr,\n",
    "                    beta_1=beta_1,\n",
    "                    beta_2=beta_2,\n",
    "                    epsilon=epsilon,\n",
    "                    amsgrad=amsgrad,\n",
    "                    name=name,\n",
    "                )\n",
    "                \n",
    "                # Baseline model\n",
    "                baseline = RolloutBaseline(\n",
    "                    model             = model_amd,\n",
    "                    filename          = None,\n",
    "                    from_checkpoint   = False,\n",
    "                    path_to_checkpoint= None,\n",
    "                    wp_n_epochs       = wp_n_epochs,\n",
    "                    epoch             = epoch,\n",
    "                    num_samples       = num_samples,\n",
    "                    embedding_dim     = enc_dim,\n",
    "                    graph_size        = graph_size\n",
    "                    )\n",
    "                for _iter in iterations:\n",
    "                    print(f'Iteration: {_iter}')\n",
    "                    date = 'oct_25'\n",
    "                    filename = f'checkpoints/AMD-banchmarking-trained_on-{_iter * num_samples}-num_layers-{enc_layer}-attn-{attn_h}-embedding_dim-{enc_dim}-lr-{lr}-date-{date}.ckp'\n",
    "                    \n",
    "                    validation_dataset = create_data_on_disk(\n",
    "                        graph_size     =graph_size,\n",
    "                        num_samples    =num_samples,\n",
    "                        is_save        =False,\n",
    "                        filename       =None,\n",
    "                        is_return      =True,\n",
    "                        seed           = 42\n",
    "                    )\n",
    "                    train_model(\n",
    "                        optimizer          = optimizer,\n",
    "                        model_tf           = model_amd,\n",
    "                        baseline           = baseline,\n",
    "                        validation_dataset = validation_dataset,\n",
    "                        samples            = samples,\n",
    "                        batch              = batch,\n",
    "                        val_batch_size     = val_batch_size,\n",
    "                        start_epoch        = start_epoch,\n",
    "                        end_epoch          = _iter,\n",
    "                        from_checkpoint    = False,\n",
    "                        grad_norm_clipping = grad_norm_clipping,\n",
    "                        batch_verbose      = batch_verbose,\n",
    "                        graph_size         = graph_size,\n",
    "                        filename           = None\n",
    "                        )\n",
    "                    model_amd.save_weights(filename)\n",
    "                    # To Do: Add information about progress\n",
    "                print(f'Time for Parameter Iteration: {time.time() - start_time}')\n",
    "\n",
    "print(f'Total time: {time.time() - global_start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f851e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'attention_dynamic_model/graph_attention_encoder/init_embed_depot/kernel:0' shape=(2, 64) dtype=float32, numpy=\n",
       " array([[ 0.13315047, -0.10507061, -0.11463892, -0.1462347 ,  0.0042512 ,\n",
       "         -0.0430197 , -0.1908948 , -0.07025934,  0.08306053, -0.01930277,\n",
       "          0.12962146, -0.19577964,  0.2500617 , -0.03038784, -0.2848677 ,\n",
       "         -0.22235033,  0.20459744, -0.11801361,  0.13305368, -0.25599414,\n",
       "         -0.24735446, -0.05458432, -0.01473179,  0.2734475 , -0.11864787,\n",
       "          0.19604796, -0.2913884 , -0.1580056 , -0.18718438, -0.09049027,\n",
       "          0.02109828, -0.11962614,  0.21479948,  0.24813636,  0.19356214,\n",
       "          0.20777883, -0.25860614,  0.29386246,  0.2750997 , -0.06497433,\n",
       "          0.03031395, -0.06930444,  0.20231831,  0.14439535, -0.18665949,\n",
       "         -0.16434969, -0.23551273,  0.03986199, -0.2667672 ,  0.07536203,\n",
       "          0.08386199,  0.10153659,  0.22627957,  0.10100763, -0.0506764 ,\n",
       "         -0.14208376,  0.22061281,  0.05918619,  0.06576119,  0.02456584,\n",
       "          0.18487597, -0.02414613, -0.29254115, -0.24974793],\n",
       "        [ 0.29856783, -0.01073222, -0.18297389,  0.20851584, -0.06728144,\n",
       "         -0.16096607, -0.14693676, -0.17838697, -0.22061908,  0.2946181 ,\n",
       "          0.03819028,  0.29516914,  0.005583  , -0.01772359,  0.09784815,\n",
       "          0.18843433, -0.08740954, -0.19432563,  0.14324263, -0.14028996,\n",
       "          0.07076591,  0.05852489, -0.21976896, -0.277736  ,  0.09918686,\n",
       "         -0.05244347,  0.25794962,  0.15745768,  0.14266002, -0.13200553,\n",
       "         -0.17149265,  0.12567519,  0.08890755,  0.10518977,  0.08779509,\n",
       "          0.2547516 , -0.12316894, -0.21891196,  0.2596919 , -0.07532357,\n",
       "          0.0337984 ,  0.08205374,  0.29704136,  0.12946914,  0.00847484,\n",
       "         -0.07812239,  0.196411  , -0.29020128, -0.24523471,  0.20659383,\n",
       "         -0.06932588, -0.0331433 ,  0.30422458,  0.20982176, -0.1212429 ,\n",
       "         -0.06301827,  0.25597924,  0.0293774 ,  0.2025955 , -0.11990266,\n",
       "         -0.0108337 ,  0.13123684,  0.17060655, -0.11950228]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/init_embed_depot/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([ 2.4362605e-03,  5.1305015e-03, -4.8010345e-03, -3.4947609e-04,\n",
       "         5.5189785e-03, -2.9353558e-03, -5.9558218e-03, -5.0257528e-03,\n",
       "        -5.7196077e-03,  3.7974063e-03,  4.7031110e-03,  5.6574293e-03,\n",
       "         2.9420762e-03,  2.5100678e-03,  4.8291143e-03,  7.7925644e-05,\n",
       "         1.6637085e-03, -1.4216139e-03,  3.7337882e-03, -1.7704421e-03,\n",
       "        -4.8072743e-03, -5.0823833e-03,  4.8465929e-03,  5.8930889e-03,\n",
       "         3.7541322e-03,  4.0824278e-03,  4.1194600e-03, -4.2747953e-03,\n",
       "         4.6664802e-03,  1.5573275e-04,  2.5578416e-03, -4.6053072e-03,\n",
       "         3.7740134e-03,  5.2867271e-03,  5.0383825e-03, -3.0845483e-03,\n",
       "        -5.8030114e-03,  5.7043904e-03,  2.1677613e-03, -5.8881985e-03,\n",
       "         3.8834189e-03, -5.1140017e-03,  4.9815960e-03, -6.4892767e-05,\n",
       "         3.8866219e-03, -2.1186911e-03, -5.3711967e-03,  1.5757441e-03,\n",
       "         1.2420096e-03, -8.4862212e-04, -1.3190992e-03,  5.1150313e-03,\n",
       "         5.0686128e-03, -1.7694427e-03, -4.8084045e-03,  3.2043040e-03,\n",
       "         5.4447982e-03,  5.8546956e-03,  4.4704787e-03, -4.9708947e-03,\n",
       "        -4.4242949e-03,  1.9725065e-03,  2.7697738e-03, -5.8529596e-03],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/init_embed/kernel:0' shape=(3, 64) dtype=float32, numpy=\n",
       " array([[ 0.25054282, -0.08960246, -0.18951803,  0.07226125, -0.30607253,\n",
       "         -0.05278766, -0.03075618, -0.11840803,  0.055778  ,  0.19155374,\n",
       "          0.10698734, -0.06791746, -0.00379891, -0.17598745, -0.18125689,\n",
       "          0.00696063, -0.002984  , -0.17092665,  0.20976998, -0.03410348,\n",
       "         -0.23076631, -0.04375596,  0.01993429,  0.00714284,  0.14171003,\n",
       "         -0.01280238, -0.30164218,  0.1722992 , -0.11723905, -0.14660396,\n",
       "          0.01669092, -0.13471389, -0.10099521, -0.03214495, -0.06286983,\n",
       "         -0.04113121,  0.2495628 ,  0.02168348, -0.08850464,  0.16267025,\n",
       "          0.00066855, -0.10211381, -0.23001915, -0.27855247, -0.16491157,\n",
       "          0.29651186,  0.07284123,  0.11546683,  0.29452565,  0.0956362 ,\n",
       "         -0.21733907, -0.03440835, -0.28294227, -0.06549401,  0.14921004,\n",
       "          0.2316376 , -0.21598236, -0.2723794 , -0.06083782,  0.25986436,\n",
       "          0.25063697,  0.00482505, -0.0628631 ,  0.17972657],\n",
       "        [-0.05742392, -0.18922642, -0.12760057,  0.00859398,  0.080952  ,\n",
       "         -0.07423347, -0.28783962,  0.00765799, -0.08201639, -0.2270921 ,\n",
       "         -0.07952564,  0.16817403, -0.04744954, -0.15346327,  0.18001048,\n",
       "         -0.30184802, -0.23997463,  0.16094677,  0.29812413,  0.23845077,\n",
       "         -0.13575219,  0.01373572, -0.03282909,  0.15411279,  0.17876136,\n",
       "         -0.1948266 , -0.03462687,  0.01133888, -0.04229487, -0.01284332,\n",
       "          0.3039923 , -0.25671872, -0.24947147,  0.25583172,  0.19467963,\n",
       "         -0.08665296,  0.25941917,  0.04061823, -0.22865503, -0.16584352,\n",
       "         -0.2992726 , -0.12913096,  0.18978062, -0.01912758,  0.10057811,\n",
       "         -0.17966244,  0.0766646 , -0.04670481, -0.06104422,  0.28329512,\n",
       "          0.27540874,  0.04293353, -0.21366245,  0.21110244,  0.24627024,\n",
       "         -0.1219032 ,  0.16236933,  0.28126723,  0.01624278,  0.07610016,\n",
       "         -0.10551105, -0.23403616, -0.21945679, -0.2208619 ],\n",
       "        [-0.24158008, -0.08544517,  0.09813594, -0.12542596, -0.02465154,\n",
       "         -0.0721931 ,  0.21447705, -0.2095813 , -0.20153238, -0.09673829,\n",
       "         -0.06876754, -0.14750004,  0.08428656,  0.11971918, -0.2649159 ,\n",
       "         -0.20346124,  0.12311572, -0.00577531,  0.04798655, -0.16053991,\n",
       "          0.00326483,  0.03167554,  0.09343081, -0.09893597,  0.11748002,\n",
       "         -0.04001651,  0.0007779 , -0.18658654,  0.18128154, -0.14755815,\n",
       "          0.0417559 , -0.05086387,  0.1565028 ,  0.17124656, -0.13024323,\n",
       "         -0.07790296, -0.17643331,  0.23168066,  0.17397594, -0.2954925 ,\n",
       "          0.18090011, -0.05783491,  0.04283993,  0.04069612, -0.15765017,\n",
       "          0.10310861,  0.12258239, -0.21235518, -0.2778709 , -0.1193392 ,\n",
       "          0.24209146, -0.2600481 ,  0.14638479, -0.13564922,  0.05524964,\n",
       "         -0.05679589, -0.04166917, -0.19772649,  0.14913775,  0.24489622,\n",
       "          0.21772717,  0.00935825,  0.2338408 , -0.13064785]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/init_embed/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([ 0.00334347, -0.00537147, -0.00115316, -0.00113867, -0.00549365,\n",
       "         0.00695307, -0.0084675 , -0.00734892, -0.00341665, -0.00721944,\n",
       "        -0.00532994, -0.00031249,  0.00434913, -0.0064786 , -0.00521768,\n",
       "        -0.00440985,  0.00699655, -0.00682547, -0.00138897, -0.00509418,\n",
       "        -0.00767888,  0.00086326,  0.00330856,  0.00627343, -0.00049683,\n",
       "         0.00460883, -0.00523455,  0.00530495,  0.00733746, -0.00607524,\n",
       "         0.00644608,  0.00187707, -0.00628187,  0.00749426, -0.00206555,\n",
       "        -0.00757681,  0.00655959,  0.0050948 , -0.00790504, -0.00546697,\n",
       "        -0.00581867, -0.00312662,  0.00202114, -0.0068575 , -0.00497263,\n",
       "        -0.00559989,  0.00135648, -0.00710014,  0.00223368, -0.00419499,\n",
       "         0.00612861, -0.00141855, -0.0059389 ,  0.00384201,  0.00136021,\n",
       "         0.00074698,  0.0005713 ,  0.00644506,  0.00618808,  0.00667324,\n",
       "         0.00660997, -0.00735443, -0.00747519, -0.0069059 ], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/multi_head_attention_layer/MHA/dense/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[ 0.11233921,  0.15655224,  0.01674786, ...,  0.13411282,\n",
       "          0.20631818, -0.09472149],\n",
       "        [ 0.17498693,  0.1589741 ,  0.15909693, ...,  0.07613545,\n",
       "          0.01029404,  0.13884564],\n",
       "        [ 0.12765396,  0.12680447, -0.01796964, ...,  0.05600087,\n",
       "          0.15506211, -0.03645836],\n",
       "        ...,\n",
       "        [-0.14967974,  0.19629075,  0.09700758, ...,  0.20815383,\n",
       "          0.2075146 , -0.07434433],\n",
       "        [ 0.03080553,  0.01859823, -0.03655237, ..., -0.06257747,\n",
       "          0.02020389, -0.02335091],\n",
       "        [-0.11205474,  0.02473222,  0.18426734, ..., -0.07812815,\n",
       "         -0.06663676,  0.07368403]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/multi_head_attention_layer/MHA/dense_1/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[ 0.07902114, -0.07334695, -0.2038906 , ..., -0.1157456 ,\n",
       "         -0.03151952, -0.11344793],\n",
       "        [-0.09379623,  0.17064175,  0.17024091, ...,  0.17503527,\n",
       "         -0.10022762, -0.01133804],\n",
       "        [ 0.00234249,  0.00104578, -0.13811046, ..., -0.13684306,\n",
       "          0.08772442,  0.11634893],\n",
       "        ...,\n",
       "        [-0.1437385 ,  0.00897148, -0.00688824, ..., -0.21405579,\n",
       "          0.0271986 ,  0.16539538],\n",
       "        [-0.18116748, -0.01529857,  0.01353117, ...,  0.11863245,\n",
       "         -0.1021698 ,  0.03782204],\n",
       "        [-0.04736003,  0.17910893, -0.1759903 , ...,  0.12602194,\n",
       "         -0.06433169,  0.18168911]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/multi_head_attention_layer/MHA/dense_2/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[ 0.08924921, -0.1644499 ,  0.1493269 , ...,  0.05654075,\n",
       "         -0.06874252, -0.03938138],\n",
       "        [ 0.07440995,  0.12911555, -0.09091526, ...,  0.03508995,\n",
       "         -0.01297511, -0.16615595],\n",
       "        [-0.02796186, -0.11822037, -0.20723774, ...,  0.12016363,\n",
       "          0.06678687,  0.06998863],\n",
       "        ...,\n",
       "        [-0.18535207, -0.1817016 , -0.20335087, ...,  0.16466501,\n",
       "          0.04205607,  0.05195909],\n",
       "        [ 0.11104471, -0.01897325, -0.13401437, ...,  0.12462559,\n",
       "          0.05174494, -0.09683315],\n",
       "        [-0.0362631 , -0.14057212, -0.09468649, ..., -0.0269691 ,\n",
       "          0.02221643,  0.21474756]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/multi_head_attention_layer/MHA/dense_3/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[ 0.14285028, -0.01051242, -0.20386286, ...,  0.19052869,\n",
       "          0.04167781,  0.06341562],\n",
       "        [ 0.18766724, -0.1498756 , -0.15998045, ..., -0.0931341 ,\n",
       "          0.12402698, -0.02468546],\n",
       "        [-0.18185958, -0.01160702,  0.01270542, ...,  0.0357246 ,\n",
       "          0.06281853,  0.17692782],\n",
       "        ...,\n",
       "        [-0.1921001 , -0.07186186, -0.03790802, ...,  0.04784742,\n",
       "          0.20367369, -0.11514016],\n",
       "        [ 0.04849397,  0.18088059,  0.19820772, ...,  0.06389329,\n",
       "         -0.12127341, -0.03885438],\n",
       "        [ 0.1945691 , -0.05354454,  0.06661297, ...,  0.18663174,\n",
       "         -0.16117798,  0.01007413]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/multi_head_attention_layer/ff1/kernel:0' shape=(64, 512) dtype=float32, numpy=\n",
       " array([[ 0.0132269 , -0.03286906,  0.06966782, ...,  0.00560875,\n",
       "          0.0764178 , -0.07172398],\n",
       "        [ 0.02108807,  0.00786927,  0.02792911, ...,  0.02216348,\n",
       "         -0.06496964, -0.08105152],\n",
       "        [-0.07673933, -0.02627863, -0.0503926 , ...,  0.04385154,\n",
       "          0.03165489,  0.08423536],\n",
       "        ...,\n",
       "        [-0.02400682,  0.05382407, -0.00398124, ..., -0.02355716,\n",
       "         -0.00825818, -0.08973011],\n",
       "        [-0.01790436, -0.02799979, -0.09435677, ...,  0.08807014,\n",
       "          0.01948206,  0.02972416],\n",
       "        [-0.08014834, -0.02058049, -0.06754671, ..., -0.07046255,\n",
       "         -0.08683674, -0.06599389]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/multi_head_attention_layer/ff1/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([-7.89382961e-04,  5.57364710e-03, -2.69837474e-04,  4.52283351e-03,\n",
       "        -4.13207058e-03, -6.22508465e-04,  1.23866368e-03,  4.45953757e-03,\n",
       "        -3.54412571e-03,  3.21127987e-03,  7.36296969e-03, -4.71062632e-03,\n",
       "         5.26855700e-03, -1.76788995e-03, -3.98642104e-03, -3.91099323e-03,\n",
       "         3.24029173e-03,  6.32284535e-03, -5.42060472e-03, -4.18867823e-03,\n",
       "         7.83710799e-04, -8.95257865e-04,  6.15827227e-03,  4.46213846e-04,\n",
       "        -4.73531848e-03,  3.70200677e-03,  2.43020873e-03, -3.11167142e-03,\n",
       "        -1.43333606e-03, -2.31167115e-03, -1.16491201e-03,  1.19112374e-03,\n",
       "        -6.04127627e-03,  5.86831803e-03,  5.54488786e-03, -1.91785314e-03,\n",
       "        -1.88307650e-03,  3.29878973e-03,  6.56512985e-03,  4.97299293e-03,\n",
       "         4.18167328e-03, -9.11568757e-04,  1.57224305e-03, -2.79468554e-03,\n",
       "        -3.45582399e-03, -1.84896763e-03,  1.94516720e-03, -1.12515445e-04,\n",
       "         4.41228691e-03,  2.44354550e-03,  1.10011410e-04, -4.65524057e-03,\n",
       "         4.53499286e-03,  2.23986618e-03, -1.00907171e-03, -4.35759639e-03,\n",
       "         2.46128440e-03,  5.70450968e-04, -1.93828251e-03,  3.71653750e-03,\n",
       "         6.78113475e-03, -5.35064796e-03,  5.78056229e-03,  7.37691578e-03,\n",
       "         4.34266822e-03,  0.00000000e+00, -3.02547915e-03,  2.45636934e-03,\n",
       "        -2.79534899e-04,  5.93033433e-03,  5.87134017e-03,  5.93614823e-04,\n",
       "         4.99093346e-03, -4.55304096e-03, -3.87038500e-03,  2.38798000e-03,\n",
       "         3.36212176e-03, -6.12112926e-03,  3.08343885e-03, -4.06504981e-03,\n",
       "        -2.62350799e-03,  1.09024761e-04,  3.10348044e-03,  1.06584001e-03,\n",
       "        -4.29685460e-03, -5.93061908e-04,  2.40251864e-03,  0.00000000e+00,\n",
       "        -2.32504890e-03,  7.79895490e-05, -3.96437384e-03,  5.43852942e-03,\n",
       "        -5.75993117e-03,  0.00000000e+00, -2.13523791e-03,  1.42188196e-03,\n",
       "        -2.71320250e-03,  4.98968130e-03,  2.49430561e-03,  5.82512282e-03,\n",
       "        -1.96525641e-03, -2.04539229e-03,  4.47164103e-03,  6.35254523e-03,\n",
       "         2.32757465e-03, -4.90016428e-05,  5.33031626e-03,  5.67936432e-03,\n",
       "         4.34005447e-03, -1.93217638e-04, -1.28709493e-04, -4.48143017e-03,\n",
       "         6.13844814e-03,  6.85717911e-03,  2.45209690e-03, -2.50298856e-03,\n",
       "         6.77767443e-03, -1.30372599e-03,  6.09833607e-03,  2.80569308e-04,\n",
       "        -1.72314059e-03,  2.97334092e-03, -7.22598983e-04,  5.98093448e-03,\n",
       "         6.59840927e-03, -1.57795765e-03, -1.57760130e-03, -1.99174508e-03,\n",
       "        -3.05034802e-03, -2.55427812e-03, -1.20120391e-03,  4.30955971e-03,\n",
       "         4.71779611e-03, -2.78536673e-03,  2.74952548e-03,  6.38775621e-03,\n",
       "         2.23832135e-03,  2.61729490e-03,  5.67870168e-03,  1.81862444e-04,\n",
       "         2.35376391e-03,  7.03263853e-04,  8.98451428e-04, -2.89593154e-04,\n",
       "         5.64787118e-03, -1.15105882e-03,  6.21367898e-03, -3.97684565e-03,\n",
       "         6.39863638e-03,  5.12509374e-03, -3.27882660e-03,  6.10004179e-03,\n",
       "        -7.45846657e-04, -3.09646362e-03, -9.48902161e-05, -5.56164980e-03,\n",
       "         5.85431373e-03,  5.14235254e-03, -3.11067468e-03,  4.96500777e-03,\n",
       "         5.62324096e-03, -2.39860290e-03, -1.76730019e-03,  3.39454971e-03,\n",
       "         2.97411904e-03,  1.54100615e-03,  5.80180343e-03,  3.55031085e-03,\n",
       "         7.34706083e-03, -8.98332626e-04,  4.82628308e-03, -8.85926653e-04,\n",
       "         5.88901388e-03,  1.59161037e-03, -3.12016375e-04, -1.75994309e-03,\n",
       "        -5.23813069e-03, -4.14950261e-03,  4.60139359e-04,  3.64571140e-04,\n",
       "         7.33525446e-03,  2.44202698e-03,  5.80709334e-03,  4.52013826e-03,\n",
       "         6.58838020e-04,  7.51318783e-03,  6.18949486e-03, -3.60994809e-03,\n",
       "         9.76815936e-04, -3.46895377e-03, -1.53128069e-03, -2.63640191e-03,\n",
       "         2.90963124e-03,  7.33335922e-03, -4.09343746e-03, -2.63236044e-03,\n",
       "         3.55724758e-03,  5.80363907e-03,  3.86387296e-03,  6.24864269e-03,\n",
       "        -2.19105231e-03,  8.40161648e-03,  1.69965823e-03,  6.18091726e-04,\n",
       "        -4.37629130e-03, -3.32766608e-03,  9.91652953e-04, -1.65048067e-03,\n",
       "         6.89579872e-03,  3.51011846e-03, -2.14784313e-03,  6.53049024e-03,\n",
       "         6.54320815e-04, -7.52619875e-04,  3.75290099e-03,  6.94744103e-03,\n",
       "         6.54952601e-03,  2.37637223e-03,  4.09390079e-03,  4.85150842e-03,\n",
       "        -2.82683712e-03,  1.29082659e-03, -2.56831874e-03, -7.68286991e-04,\n",
       "         8.74813588e-04,  6.92699151e-03,  4.22505755e-03, -3.77683109e-03,\n",
       "         5.02792979e-03,  5.99301420e-04, -3.78212682e-03,  3.61539866e-03,\n",
       "         5.47799282e-03, -5.28874015e-03, -2.88993772e-03,  7.35768070e-03,\n",
       "         6.23328495e-04, -2.83619436e-03,  4.56513464e-03,  7.99707323e-03,\n",
       "        -4.68860241e-03,  4.03396599e-03,  1.12055463e-03, -1.07931346e-03,\n",
       "         4.43052594e-03, -1.18894549e-03, -4.35948186e-03,  4.99377446e-03,\n",
       "        -3.69948335e-03,  2.45233416e-03,  5.17182751e-03, -2.23526801e-03,\n",
       "        -6.95808965e-04,  4.95612575e-03, -4.16785292e-03,  8.77299113e-04,\n",
       "         6.17634598e-03,  4.28183004e-03, -1.29820826e-03, -1.05154922e-03,\n",
       "         4.18917462e-03,  6.20837929e-03,  6.95712212e-03,  6.31668791e-03,\n",
       "        -6.97508035e-03,  3.30585218e-03,  5.58118522e-03, -1.12083810e-03,\n",
       "         8.23609880e-04,  4.84069018e-03, -5.54131329e-05,  3.13660130e-03,\n",
       "         2.99390522e-03, -6.46313129e-04, -3.86047899e-03,  5.22269821e-03,\n",
       "         7.44091021e-03, -2.73141195e-03,  5.85937500e-03,  5.13146492e-03,\n",
       "         6.64387830e-03, -2.37136544e-03,  5.99984452e-03,  5.77894738e-03,\n",
       "        -2.75566289e-03,  4.67289891e-03,  1.40412606e-03, -5.48547180e-03,\n",
       "        -9.99366865e-04, -3.08433408e-03, -3.01203248e-03,  6.60262490e-03,\n",
       "         6.65624335e-04, -4.32509836e-03,  2.97148665e-03,  1.05149426e-04,\n",
       "        -2.18761503e-03, -5.74743841e-03, -9.33089119e-04,  4.20882180e-03,\n",
       "         1.83289929e-03,  1.88906817e-03,  6.11022860e-03,  4.45258664e-03,\n",
       "         3.15799238e-03, -4.73176874e-03,  6.67783851e-03,  2.86712637e-03,\n",
       "         2.83716060e-03,  5.67767862e-03, -2.88064522e-03,  5.76534821e-03,\n",
       "        -5.86824666e-04,  4.27094812e-04, -3.13375075e-03,  6.48947014e-03,\n",
       "         6.53778249e-03, -8.77245504e-04, -1.57793623e-03,  1.19136134e-03,\n",
       "        -1.06653385e-03,  4.69673611e-03, -4.52857278e-03,  4.02310584e-03,\n",
       "         4.96531790e-03,  6.55473256e-03,  3.02141765e-03,  6.02580700e-03,\n",
       "         6.00072369e-03, -4.02195565e-03, -4.51877480e-03, -9.65452986e-04,\n",
       "        -3.61963362e-03,  5.76327229e-03, -2.81691598e-03, -5.40692490e-05,\n",
       "        -1.21972722e-03, -4.36326116e-03,  3.97928525e-03, -6.43048296e-03,\n",
       "         2.10395129e-03, -1.63636648e-03,  7.02824304e-03,  5.33721410e-03,\n",
       "         2.98259617e-03, -2.66161049e-03,  7.33831059e-03,  5.21627441e-03,\n",
       "         4.99301217e-03,  2.22839182e-04,  5.47331292e-03, -1.94830413e-03,\n",
       "        -4.57723479e-04,  5.97476726e-03,  1.76074484e-03,  1.07289944e-03,\n",
       "         1.17630209e-03,  4.19778377e-03, -6.50175149e-04, -3.52527737e-03,\n",
       "        -4.48252028e-03,  3.40777636e-03, -3.62507017e-05,  5.20160189e-03,\n",
       "         1.95049937e-03,  4.19045566e-04,  3.70825897e-03, -5.32684987e-03,\n",
       "         1.79274590e-03,  2.81429105e-03,  0.00000000e+00, -2.44124932e-03,\n",
       "        -3.65426560e-04,  2.98174797e-03, -5.79028332e-04, -3.07873264e-03,\n",
       "        -3.13341594e-03,  3.51912458e-03,  3.50315124e-03,  7.70381093e-03,\n",
       "         0.00000000e+00, -7.31703080e-03,  1.68142212e-03,  4.16248804e-03,\n",
       "         3.95146059e-03,  2.31015799e-03,  5.96636068e-03, -4.82010044e-04,\n",
       "         6.67569926e-03,  5.35007473e-03,  3.35569237e-03, -3.03472835e-03,\n",
       "         1.98809127e-03,  3.88042908e-03,  6.89450710e-04, -2.39732279e-03,\n",
       "         3.64985643e-03, -2.95456080e-03, -9.98216332e-04,  1.37214968e-03,\n",
       "        -1.79770205e-03, -1.78229157e-03,  1.79354975e-03,  1.83148182e-03,\n",
       "         7.11993687e-03,  5.81949577e-03, -5.89263812e-03,  4.00842447e-03,\n",
       "         7.42368447e-03,  5.28260693e-03, -1.17144641e-03,  5.25580905e-03,\n",
       "         1.79931681e-04, -3.37713747e-03, -3.12945992e-03, -3.25687201e-04,\n",
       "        -1.77029066e-03, -3.64336814e-03,  2.46329320e-04, -5.61086228e-04,\n",
       "         5.31443022e-03, -8.02237715e-04,  2.84935371e-03,  5.04781771e-03,\n",
       "        -4.17429314e-04,  1.88665092e-03, -4.09437483e-03, -3.05311545e-03,\n",
       "         2.34802207e-03,  5.19051077e-03, -4.92784148e-03,  8.88533366e-04,\n",
       "         6.35695877e-03, -4.32643434e-03,  4.52652946e-03,  2.26205870e-04,\n",
       "        -2.15217238e-03,  6.39876351e-03, -8.03192845e-04,  6.98066643e-03,\n",
       "        -2.65526236e-03,  3.78675899e-03, -2.88421032e-03,  1.77014712e-03,\n",
       "        -2.80313171e-03, -8.30559991e-04, -2.44139344e-03,  4.46670642e-03,\n",
       "         1.95796485e-03, -1.53527176e-03, -5.65166993e-04,  2.11191969e-03,\n",
       "         1.10963243e-03,  4.09146165e-03, -3.42527009e-03, -3.40272393e-03,\n",
       "        -1.49624934e-03, -1.01500633e-03,  0.00000000e+00, -3.82201630e-03,\n",
       "         3.51204770e-03,  6.39714472e-07, -1.28481630e-03, -2.77053635e-03,\n",
       "         4.78483131e-03,  3.25784320e-03, -3.18861008e-03, -1.56268812e-04,\n",
       "         2.52208184e-03,  1.24785060e-03,  6.78522652e-03,  4.49704565e-03,\n",
       "        -4.45383415e-03,  2.97471578e-03,  5.49351098e-03,  6.24811370e-03,\n",
       "         6.50578458e-03,  5.53135620e-03,  3.98962619e-03,  7.65960338e-03,\n",
       "         3.98744410e-03,  3.82773869e-04, -7.03547569e-03,  5.55970939e-03,\n",
       "        -2.00567767e-04,  8.28694829e-06, -1.13665499e-03, -4.26115061e-04,\n",
       "         5.17520122e-03,  2.62685848e-04, -2.70510849e-04, -3.94848641e-03,\n",
       "         1.84509240e-03, -1.70448868e-04,  3.64418025e-04, -2.00178381e-03,\n",
       "         9.37266799e-04,  2.84568849e-03, -2.04023486e-03, -7.27231055e-03,\n",
       "         4.89893230e-03,  5.06112957e-03,  1.59051723e-03, -1.73433649e-03,\n",
       "         6.88570971e-03,  5.88938303e-04, -4.37226566e-03, -2.52764276e-03,\n",
       "         2.12951377e-03,  0.00000000e+00,  2.82140891e-03, -8.82119115e-04],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/multi_head_attention_layer/ff2/kernel:0' shape=(512, 64) dtype=float32, numpy=\n",
       " array([[-0.05155706, -0.07626928, -0.03543542, ..., -0.01727616,\n",
       "         -0.02042669, -0.06887504],\n",
       "        [ 0.0874842 ,  0.03415551, -0.05398459, ...,  0.03380777,\n",
       "          0.03689468, -0.10498954],\n",
       "        [ 0.10780828, -0.02551128,  0.09153964, ...,  0.06880326,\n",
       "         -0.08708938,  0.07381879],\n",
       "        ...,\n",
       "        [ 0.00993145,  0.1018097 ,  0.03004518, ...,  0.05996679,\n",
       "         -0.08824988, -0.00262799],\n",
       "        [-0.07447299,  0.06076661,  0.06922302, ..., -0.00034777,\n",
       "          0.06238799,  0.06302475],\n",
       "        [-0.08037786, -0.03419504,  0.09273946, ..., -0.0909081 ,\n",
       "         -0.06555563,  0.07450447]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/graph_attention_encoder/multi_head_attention_layer/ff2/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([ 0.00681572, -0.00272541, -0.00524251, -0.00538637, -0.00223172,\n",
       "         0.00573991, -0.00665704,  0.00592857, -0.0062618 , -0.00359597,\n",
       "         0.00272643,  0.00777612,  0.00599432, -0.0040381 ,  0.00073497,\n",
       "         0.00120501,  0.00501763, -0.00466436,  0.0046752 ,  0.00265021,\n",
       "        -0.00666318, -0.00416996,  0.00526207,  0.0065224 ,  0.00142127,\n",
       "         0.00555599, -0.00598998, -0.00394941,  0.00645724,  0.00506525,\n",
       "        -0.00202896,  0.00155244,  0.00639351, -0.00305032, -0.00568886,\n",
       "        -0.00524877,  0.00039868,  0.00515046,  0.00248804, -0.00642372,\n",
       "        -0.00332921, -0.00613498,  0.00084208, -0.00344563, -0.00385524,\n",
       "        -0.00606042,  0.00308992,  0.00257678, -0.00651119, -0.00634853,\n",
       "         0.00328717, -0.00589813, -0.00127545,  0.00472235, -0.00481298,\n",
       "         0.00626776,  0.00373858,  0.00585713, -0.00076845,  0.00133299,\n",
       "        -0.00021977, -0.00060526, -0.00704514, -0.0065967 ], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/wq_context/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[-0.00921287,  0.08636627, -0.07706685, ..., -0.13354164,\n",
       "         -0.19649883, -0.02051484],\n",
       "        [ 0.11972807,  0.05054696, -0.1809017 , ..., -0.10977764,\n",
       "         -0.13856632,  0.21402363],\n",
       "        [-0.04962814, -0.00280989, -0.0790175 , ...,  0.10348566,\n",
       "         -0.05626724,  0.08051809],\n",
       "        ...,\n",
       "        [ 0.01077368, -0.17616417, -0.10341258, ...,  0.01550531,\n",
       "          0.14341791,  0.19030869],\n",
       "        [ 0.16662692, -0.02686212,  0.03267724, ...,  0.12902944,\n",
       "         -0.09876247,  0.03998608],\n",
       "        [-0.02090489,  0.01407109,  0.00756439, ..., -0.15285988,\n",
       "          0.02673747,  0.13750069]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/wq_step_context/kernel:0' shape=(65, 64) dtype=float32, numpy=\n",
       " array([[ 0.18199302,  0.21311247,  0.20681752, ...,  0.19356205,\n",
       "         -0.12125512,  0.15677567],\n",
       "        [ 0.15397936,  0.09881143, -0.09964214, ...,  0.21477856,\n",
       "          0.14752825,  0.02625256],\n",
       "        [-0.02600542,  0.06292174,  0.19021311, ...,  0.15474994,\n",
       "         -0.15719107,  0.02156052],\n",
       "        ...,\n",
       "        [-0.10819542, -0.11979706,  0.17402591, ...,  0.1129925 ,\n",
       "         -0.19646631,  0.00281204],\n",
       "        [-0.14776099, -0.09845211, -0.16437328, ..., -0.13292718,\n",
       "         -0.1920108 , -0.1668964 ],\n",
       "        [-0.17462638,  0.11894748, -0.04205089, ...,  0.17083973,\n",
       "          0.03455742,  0.01042322]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/wk/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[ 0.09940057,  0.12321298,  0.20209947, ...,  0.01703168,\n",
       "          0.13345169,  0.08567547],\n",
       "        [-0.04626897, -0.08382517, -0.03496516, ..., -0.19221207,\n",
       "         -0.08297633, -0.20985694],\n",
       "        [-0.05075504, -0.1482906 ,  0.12311024, ...,  0.19880448,\n",
       "          0.03382631, -0.2006208 ],\n",
       "        ...,\n",
       "        [ 0.18748918, -0.19874555,  0.10013724, ..., -0.0104642 ,\n",
       "         -0.17991135,  0.1331519 ],\n",
       "        [-0.10425013,  0.03093045,  0.16880299, ...,  0.21217078,\n",
       "         -0.08141953,  0.08714598],\n",
       "        [-0.00979726,  0.08545151, -0.11629111, ...,  0.18900163,\n",
       "          0.09519725,  0.02197516]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/wk_tanh/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[-0.02901585, -0.00616377,  0.0024471 , ...,  0.04573724,\n",
       "         -0.20594603,  0.02927546],\n",
       "        [ 0.06203616, -0.08545409, -0.16160852, ...,  0.11371305,\n",
       "          0.02564619, -0.17947344],\n",
       "        [-0.11017781,  0.09817371, -0.16978748, ...,  0.13550588,\n",
       "         -0.13955851,  0.05614147],\n",
       "        ...,\n",
       "        [-0.16924341,  0.03060922,  0.01410244, ...,  0.05883509,\n",
       "          0.01873683,  0.00621597],\n",
       "        [ 0.03628874, -0.18767379,  0.16053343, ..., -0.03726292,\n",
       "         -0.00263871, -0.20332156],\n",
       "        [-0.18627523, -0.07057748,  0.04775314, ..., -0.02645946,\n",
       "          0.20149437, -0.19248804]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/wv/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[ 0.19012913, -0.20340462,  0.09130611, ..., -0.02307775,\n",
       "         -0.00102866,  0.00081637],\n",
       "        [ 0.18829972, -0.14588411,  0.15208781, ..., -0.14461103,\n",
       "         -0.16422333, -0.11264797],\n",
       "        [-0.14715047,  0.06781379,  0.05256693, ...,  0.18230446,\n",
       "          0.11665358, -0.01373222],\n",
       "        ...,\n",
       "        [ 0.09450798,  0.08789875,  0.0625893 , ...,  0.03596695,\n",
       "          0.02668236,  0.03956084],\n",
       "        [ 0.14254642,  0.0146912 ,  0.2175613 , ..., -0.1272361 ,\n",
       "         -0.20816515,  0.004704  ],\n",
       "        [-0.2000355 , -0.07504146,  0.03747417, ...,  0.16547613,\n",
       "          0.0889928 , -0.04193322]], dtype=float32)>,\n",
       " <tf.Variable 'attention_dynamic_model/w_out/kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[-0.14517276, -0.14140198, -0.0984268 , ...,  0.04711322,\n",
       "         -0.1826901 , -0.08127772],\n",
       "        [-0.02888352, -0.02070981, -0.09550471, ..., -0.04437956,\n",
       "          0.0361863 ,  0.03519868],\n",
       "        [ 0.02024562,  0.21957055,  0.03824954, ..., -0.11937693,\n",
       "         -0.14838919, -0.10498024],\n",
       "        ...,\n",
       "        [ 0.0134672 ,  0.1249399 ,  0.00548475, ..., -0.16290785,\n",
       "          0.13437945,  0.07522617],\n",
       "        [ 0.11047799, -0.0344455 ,  0.13339584, ...,  0.17157899,\n",
       "          0.20419465, -0.04952194],\n",
       "        [-0.19045661,  0.08266044, -0.03489299, ...,  0.13621004,\n",
       "         -0.00703187, -0.20917346]], dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_amd.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39f4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
