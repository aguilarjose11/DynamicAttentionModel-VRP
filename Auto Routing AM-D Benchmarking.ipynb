{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55432bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 17:27:16.584715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "env: TF_GPU_ALLOCATOR=cuda_malloc_async\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 17:27:18.041874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-24 17:27:18.072034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-24 17:27:18.072207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "from attention_dynamic_model import AttentionDynamicModel\n",
    "import tensorflow as tf\n",
    "import time\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "%env TF_GPU_ALLOCATOR=cuda_malloc_async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d938c",
   "metadata": {},
   "source": [
    "Benchmarking AM-D\n",
    "=================\n",
    "\n",
    "This experiment will train an AM-D model based on the following hyperparameters:\n",
    "\n",
    "| Hyperparameter | Values |\n",
    "| ---: | :--- |\n",
    "| Learning Rate | 1e-4, 5e-5 1e-5 |\n",
    "| Embedding Dimensions | 64, 128, 256 |\n",
    "| Attention Heads | 6, 8, 10 |\n",
    "| Encoder Layers | 1, 2, 3, 4 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rates = [1e-4, 5e-5, 1e-5]\n",
    "enc_dims    = [64, 128, 256]\n",
    "attn_heads  = [6, 8, 10]\n",
    "enc_layers  = [1, 2, 3, 4]\n",
    "iterations  = [2, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "''' Constant Parameters '''\n",
    "# AM-D Constants\n",
    "tanh_clipping = 10\n",
    "# Optimizer Constants (Adam)\n",
    "beta_1        = 0.9\n",
    "beta_2        = 0.999\n",
    "epsilon       = 1e-07\n",
    "amsgrad       = False\n",
    "name          = \"Adam\"\n",
    "# Environment Constants\n",
    "graph_size         = 20\n",
    "# Rollout Baseline Constants\n",
    "wp_n_epochs        = 5\n",
    "epoch              = 0\n",
    "num_samples        = 10_000 #???\n",
    "warmup_exp_beta    = 0.8\n",
    "# Training Constants\n",
    "samples            = 50_000 #1_280_000 # 512\n",
    "batch              = 64\n",
    "val_batch_size     = 1_000\n",
    "start_epoch        = 0\n",
    "grad_norm_clipping = 1.0\n",
    "batch_verbose      = 1_000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for lr in learn_rates:\n",
    "    for enc_dim in enc_dims:\n",
    "        for attn_h in attn_heads:\n",
    "            for enc_layer in enc_layers:\n",
    "                # Build AM-D Model\n",
    "                model_amd = AttentionDynamicModel(\n",
    "                    embedding_dim  =enc_dim,\n",
    "                    n_encode_layers=enc_layer,\n",
    "                    n_heads        =attn_h,\n",
    "                    tanh_clipping  =tanh_clipping\n",
    "                )\n",
    "                model_amd.set_decode_type('sampling')\n",
    "                \n",
    "                # Create Optimizer\n",
    "                optimizer = Adam(\n",
    "                    learning_rate=lr,\n",
    "                    beta_1=beta_1,\n",
    "                    beta_2=beta_2,\n",
    "                    epsilon=epsilon,\n",
    "                    amsgrad=amsgrad,\n",
    "                    name=name,\n",
    "                )\n",
    "                \n",
    "                # Baseline model\n",
    "                baseline = RolloutBaseline(\n",
    "                    model             = model_amd,\n",
    "                    filename          = 'VRP_{}_{}'.format(graph_size, strftime(\"%Y-%m-%d\", gmtime())),\n",
    "                    from_checkpoint   = False,\n",
    "                    path_to_checkpoint= None,\n",
    "                    wp_n_epochs       = wp_n_epochs,\n",
    "                    epoch             = epoch,\n",
    "                    num_samples       = num_samples,\n",
    "                    embedding_dim     = enc_dim,\n",
    "                    graph_size        = graph_size\n",
    "                    )\n",
    "                for _iter in iterations:\n",
    "                    date = 'oct_24'\n",
    "                    filename = f'checkpoints/AMD-banchmarking-trained_on-{_iter * num_samples}-num_layers-{enc_layer}-attn-{attn_h}-embedding_dim-{enc_dim}-lr-{lr}-date-{date}.ckp'\n",
    "                    validation_dataset = create_data_on_disk(\n",
    "                        graph_size     =graph_size,\n",
    "                        num_samples    =num_samples,\n",
    "                        is_save        =False,\n",
    "                        filename       =None,\n",
    "                        is_return      =True,\n",
    "                        seed           = 42\n",
    "                    )\n",
    "                    train_model(\n",
    "                        optimizer          = optimizer,\n",
    "                        model_tf           = model_amd,\n",
    "                        baseline           = baseline,\n",
    "                        validation_dataset = validation_dataset,\n",
    "                        samples            = samples,\n",
    "                        batch              = batch,\n",
    "                        val_batch_size     = val_batch_size,\n",
    "                        start_epoch        = start_epoch,\n",
    "                        end_epoch          = _iter,\n",
    "                        from_checkpoint    = False,\n",
    "                        grad_norm_clipping = grad_norm_clipping,\n",
    "                        batch_verbose      = batch_verbose,\n",
    "                        graph_size         = graph_size,\n",
    "                        filename           = None\n",
    "                        )\n",
    "                    model_amd.save_weights(filename)\n",
    "                    # To Do: Add information about progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
